{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Chapter 2 Tutorial: Understanding LLMs and Pre-training\n",
    "\n",
    "In this tutorial, we will explore the mechanics of LLM architectures, with an emphasis on the differences between masked models and causal models. In the first section, we'll examine some existing pretrained models to understand how they produce their outputs. Once we've demonstrated how LLM's are able to do what they do, we will then run an abbreviated training loop to provide a glimpse into the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p66qIkzd7wz"
   },
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zGW64FX-d7wz",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:15:25.032378Z",
     "start_time": "2025-09-16T16:15:04.301234Z"
    }
   },
   "source": [
    "!pip install datasets transformers[sentencepiece,torch]\n",
    "!pip install apache_beam"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/venv/lib/python3.12/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: transformers[sentencepiece,torch] in /opt/venv/lib/python3.12/site-packages (4.56.1)\r\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.12/site-packages (from datasets) (3.19.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.12/site-packages (from datasets) (2.3.3)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/venv/lib/python3.12/site-packages (from datasets) (21.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.12/site-packages (from datasets) (2.3.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/venv/lib/python3.12/site-packages (from datasets) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/venv/lib/python3.12/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/venv/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/venv/lib/python3.12/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/venv/lib/python3.12/site-packages (from datasets) (0.34.4)\r\n",
      "Requirement already satisfied: packaging in /opt/venv/lib/python3.12/site-packages (from datasets) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/venv/lib/python3.12/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.12/site-packages (from transformers[sentencepiece,torch]) (2025.9.1)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/venv/lib/python3.12/site-packages (from transformers[sentencepiece,torch]) (0.22.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/venv/lib/python3.12/site-packages (from transformers[sentencepiece,torch]) (0.6.2)\r\n",
      "Requirement already satisfied: torch>=2.2 in /opt/venv/lib/python3.12/site-packages (from transformers[sentencepiece,torch]) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/venv/lib/python3.12/site-packages (from transformers[sentencepiece,torch]) (1.10.1)\r\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece,torch])\r\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\r\n",
      "Collecting protobuf (from transformers[sentencepiece,torch])\r\n",
      "  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\r\n",
      "Requirement already satisfied: psutil in /opt/venv/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]) (7.0.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\r\n",
      "Requirement already satisfied: idna>=2.0 in /opt/venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/venv/lib/python3.12/site-packages (from torch>=2.2->transformers[sentencepiece,torch]) (3.4.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[sentencepiece,torch]) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.12/site-packages (from jinja2->torch>=2.2->transformers[sentencepiece,torch]) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m14.7 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\r\n",
      "Installing collected packages: sentencepiece, protobuf\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/2\u001B[0m [protobuf]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed protobuf-6.32.1 sentencepiece-0.2.1\r\n",
      "Collecting apache_beam\r\n",
      "  Downloading apache_beam-2.67.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\r\n",
      "Collecting crcmod<2.0,>=1.7 (from apache_beam)\r\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting orjson<4,>=3.9.7 (from apache_beam)\r\n",
      "  Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\r\n",
      "Collecting dill<0.3.2,>=0.3.1.1 (from apache_beam)\r\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting fastavro<2,>=0.23.6 (from apache_beam)\r\n",
      "  Downloading fastavro-1.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.7 kB)\r\n",
      "Collecting fasteners<1.0,>=0.3 (from apache_beam)\r\n",
      "  Downloading fasteners-0.20-py3-none-any.whl.metadata (4.8 kB)\r\n",
      "Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 (from apache_beam)\r\n",
      "  Downloading grpcio-1.65.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\r\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache_beam)\r\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting httplib2<0.23.0,>=0.8 (from apache_beam)\r\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from apache_beam) (4.25.1)\r\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache_beam)\r\n",
      "  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\r\n",
      "Collecting numpy<2.3.0,>=1.14.3 (from apache_beam)\r\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\r\n",
      "Collecting objsize<0.8.0,>=0.6.1 (from apache_beam)\r\n",
      "  Downloading objsize-0.7.1-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/venv/lib/python3.12/site-packages (from apache_beam) (25.0)\r\n",
      "Collecting pymongo<5.0.0,>=3.8.0 (from apache_beam)\r\n",
      "  Downloading pymongo-4.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\r\n",
      "Collecting proto-plus<2,>=1.7.1 (from apache_beam)\r\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 (from apache_beam)\r\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\r\n",
      "Collecting pydot<2,>=1.2.0 (from apache_beam)\r\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/venv/lib/python3.12/site-packages (from apache_beam) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/venv/lib/python3.12/site-packages (from apache_beam) (2025.2)\r\n",
      "Collecting redis<6,>=5.0.0 (from apache_beam)\r\n",
      "  Downloading redis-5.3.1-py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: regex>=2020.6.8 in /opt/venv/lib/python3.12/site-packages (from apache_beam) (2025.9.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.4 in /opt/venv/lib/python3.12/site-packages (from apache_beam) (2.32.5)\r\n",
      "Collecting sortedcontainers>=2.4.0 (from apache_beam)\r\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/venv/lib/python3.12/site-packages (from apache_beam) (4.15.0)\r\n",
      "Collecting zstandard<1,>=0.18.0 (from apache_beam)\r\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /opt/venv/lib/python3.12/site-packages (from apache_beam) (6.0.2)\r\n",
      "Collecting pymilvus<3.0.0,>=2.5.10 (from apache_beam)\r\n",
      "  Downloading pymilvus-2.6.1-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Collecting pyarrow<19.0.0,>=3.0.0 (from apache_beam)\r\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\n",
      "Collecting pyarrow-hotfix<1 (from apache_beam)\r\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache_beam)\r\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: six>=1.9.0 in /opt/venv/lib/python3.12/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.17.0)\r\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<0.23.0,>=0.8->apache_beam)\r\n",
      "  Downloading pyparsing-3.2.4-py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (25.3.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (2025.9.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.36.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.27.1)\r\n",
      "Requirement already satisfied: setuptools>69 in /opt/venv/lib/python3.12/site-packages (from pymilvus<3.0.0,>=2.5.10->apache_beam) (80.9.0)\r\n",
      "INFO: pip is looking at multiple versions of pymilvus to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting pymilvus<3.0.0,>=2.5.10 (from apache_beam)\r\n",
      "  Downloading pymilvus-2.6.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "  Downloading pymilvus-2.5.15-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from pymilvus<3.0.0,>=2.5.10->apache_beam)\r\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\r\n",
      "Collecting ujson>=2.0.0 (from pymilvus<3.0.0,>=2.5.10->apache_beam)\r\n",
      "  Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/venv/lib/python3.12/site-packages (from pymilvus<3.0.0,>=2.5.10->apache_beam) (2.3.2)\r\n",
      "Collecting milvus-lite>=2.4.0 (from pymilvus<3.0.0,>=2.5.10->apache_beam)\r\n",
      "  Downloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\r\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache_beam)\r\n",
      "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\r\n",
      "Collecting PyJWT>=2.9.0 (from redis<6,>=5.0.0->apache_beam)\r\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.4->apache_beam) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.4->apache_beam) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.4->apache_beam) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.4->apache_beam) (2025.8.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/venv/lib/python3.12/site-packages (from milvus-lite>=2.4.0->pymilvus<3.0.0,>=2.5.10->apache_beam) (4.67.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus<3.0.0,>=2.5.10->apache_beam) (2025.2)\r\n",
      "Downloading apache_beam-2.67.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.1/17.1 MB\u001B[0m \u001B[31m43.5 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0meta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading fastavro-1.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.5/3.5 MB\u001B[0m \u001B[31m53.9 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading fasteners-0.20-py3-none-any.whl (18 kB)\r\n",
      "Downloading grpcio-1.65.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m44.6 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\r\n",
      "Downloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\r\n",
      "Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.5/16.5 MB\u001B[0m \u001B[31m39.7 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0mm0:00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hDownloading objsize-0.7.1-py3-none-any.whl (11 kB)\r\n",
      "Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\r\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\r\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\r\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.1/40.1 MB\u001B[0m \u001B[31m40.3 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\r\n",
      "Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\r\n",
      "Downloading pymilvus-2.5.15-py3-none-any.whl (241 kB)\r\n",
      "Downloading pymongo-4.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m47.9 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\r\n",
      "Downloading pyparsing-3.2.4-py3-none-any.whl (113 kB)\r\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\r\n",
      "Downloading redis-5.3.1-py3-none-any.whl (272 kB)\r\n",
      "Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m33.0 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl (55.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.3/55.3 MB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m  \u001B[33m0:00:01\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\r\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n",
      "Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (57 kB)\r\n",
      "Building wheels for collected packages: crcmod, dill, hdfs, docopt\r\n",
      "  Building wheel for crcmod (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for crcmod: filename=crcmod-1.7-py3-none-any.whl size=18910 sha256=10600a00092e71a00b700db67b0b2b3d7495bbbbeb77ec03415e93141afbd3cf\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/08/0b/caa8b1380122cbfe6a03eaccbec0f63c67e619af4e30ca5e2a\r\n",
      "  Building wheel for dill (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78635 sha256=08ac5bc01887b8eb16cc7a09e2ccf94143c476d4b2c6c19d35db8130e6835a97\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/0e/3b/161164c3e023dd2f01045d3848c0f5f96f4dd9a058a2cd300b\r\n",
      "  Building wheel for hdfs (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34431 sha256=53d43b8939ddc85ff2c43c60abe2de0879c1a62bae93d71f2cee00effa383ef2\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/ae/d9/536505928dd3a458b206013b02625df8f12d22fa154f2bfd65\r\n",
      "  Building wheel for docopt (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=0b1dd9fb70e07ac40bd5b7b421fa66f60acbe8901a476f208baa20468f5d3ba1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\r\n",
      "Successfully built crcmod dill hdfs docopt\r\n",
      "Installing collected packages: sortedcontainers, docopt, crcmod, zstandard, ujson, python-dotenv, pyparsing, PyJWT, pyarrow-hotfix, pyarrow, protobuf, orjson, objsize, numpy, milvus-lite, jsonpickle, grpcio, fasteners, fastavro, dnspython, dill, redis, pymongo, pydot, proto-plus, httplib2, hdfs, pymilvus, apache_beam\r\n",
      "\u001B[2K  Attempting uninstall: pyarrow\r\n",
      "\u001B[2K    Found existing installation: pyarrow 21.0.0\r\n",
      "\u001B[2K    Uninstalling pyarrow-21.0.0:0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K      Successfully uninstalled pyarrow-21.0.0━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K  Attempting uninstall: protobuf0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K    Found existing installation: protobuf 6.32.1━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K    Uninstalling protobuf-6.32.1:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K      Successfully uninstalled protobuf-6.32.1━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K  Attempting uninstall: numpym\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K    Found existing installation: numpy 2.3.3━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K    Uninstalling numpy-2.3.3:m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 9/29\u001B[0m [pyarrow]\r\n",
      "\u001B[2K      Successfully uninstalled numpy-2.3.30m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13/29\u001B[0m [numpy]\r\n",
      "\u001B[2K  Attempting uninstall: dill━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m19/29\u001B[0m [dnspython]e]\r\n",
      "\u001B[2K    Found existing installation: dill 0.3.8m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m19/29\u001B[0m [dnspython]\r\n",
      "\u001B[2K    Uninstalling dill-0.3.8:━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m19/29\u001B[0m [dnspython]\r\n",
      "\u001B[2K      Successfully uninstalled dill-0.3.8[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m19/29\u001B[0m [dnspython]\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29/29\u001B[0m [apache_beam]\u001B[0m [apache_beam]\r\n",
      "\u001B[1A\u001B[2K\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed PyJWT-2.10.1 apache_beam-2.67.0 crcmod-1.7 dill-0.3.1.1 dnspython-2.8.0 docopt-0.6.2 fastavro-1.12.0 fasteners-0.20 grpcio-1.65.5 hdfs-2.7.3 httplib2-0.22.0 jsonpickle-3.4.2 milvus-lite-2.5.1 numpy-2.2.6 objsize-0.7.1 orjson-3.11.3 proto-plus-1.26.1 protobuf-5.29.5 pyarrow-18.1.0 pyarrow-hotfix-0.7 pydot-1.4.2 pymilvus-2.5.15 pymongo-4.15.0 pyparsing-3.2.4 python-dotenv-1.1.1 redis-5.3.1 sortedcontainers-2.4.0 ujson-5.11.0 zstandard-0.25.0\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:04.322161Z",
     "start_time": "2025-09-16T16:16:02.008075Z"
    }
   },
   "source": [
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGc8Si8CBbuI"
   },
   "source": [
    "## Understanding Masked LM's"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "99o28B2EGVh3",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:07.850419Z",
     "start_time": "2025-09-16T16:16:07.848294Z"
    }
   },
   "source": [
    "## The first model we will look at is BERT, which is trained with masked tokens. As an example,\n",
    "## the text below masks the word \"box\" from a well-known movie quote.\n",
    "\n",
    "text = \"Life is like a [MASK] of chocolates.\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZVii0JgSBKpU",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:26.748378Z",
     "start_time": "2025-09-16T16:16:08.437211Z"
    }
   },
   "source": [
    "## We'll now see how BERT is able to predict the missing word. We can use HuggingFace to load\n",
    "## a copy of the pretrained model and tokenizer.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TWKpRFEFA2Oz",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:36.660910Z",
     "start_time": "2025-09-16T16:16:36.654958Z"
    }
   },
   "source": [
    "## Next, we'll feed our example text into the tokenizer.\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print('input_ids:', encoded_input['input_ids'])\n",
    "print('attention_mask:', encoded_input['attention_mask'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 101, 2166, 2003, 2066, 1037,  103, 1997, 7967, 2015, 1012,  102]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o84muqUuLiwC",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:39.180601Z",
     "start_time": "2025-09-16T16:16:39.177971Z"
    }
   },
   "source": [
    "## input_ids represents the tokenized output. Each integer can be mapped back to the corresponding string.\n",
    "\n",
    "print(tokenizer.decode([7967]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolate\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1ut5R3IYbxxd",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:39.866392Z",
     "start_time": "2025-09-16T16:16:39.862849Z"
    }
   },
   "source": [
    "## The model will then receive the output of the tokenizer. We can look at the BERT model to see exactly how\n",
    "## it was constructed and what the outputs will be like.\n",
    "\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H-Xv2ujRAch4",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:40.777110Z",
     "start_time": "2025-09-16T16:16:40.741395Z"
    }
   },
   "source": [
    "## The model starts with an embedding of each of the 30,522 possible tokens into 768 dimensions, which at this\n",
    "## point is simply a representation of each token without any additional information about their relationships\n",
    "## to one another in the text. Then the encoder attention blocks are applied, updating the embeddings such that\n",
    "## they now encode each token's contribution to the chunk of text and interactions with other tokens. Notably,\n",
    "## this includes the masked tokens as well. The final stage is the language model head, which takes the embeddings\n",
    "## from the masked positions back to 30,522 dimensions. Each index of this final vector corresponds to the\n",
    "## probability that the token in that position would be the correct choice to fill the mask.\n",
    "\n",
    "\n",
    "model_output = model(**encoded_input)\n",
    "output = model_output[\"logits\"]\n",
    "\n",
    "print(output.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 30522])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4hDXB-x5tzrh",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:42.808528Z",
     "start_time": "2025-09-16T16:16:42.801384Z"
    }
   },
   "source": [
    "tokens = encoded_input['input_ids'][0].tolist()\n",
    "masked_index = tokens.index(tokenizer.mask_token_id)\n",
    "logits = output[0, masked_index, :]\n",
    "\n",
    "print(logits.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BaRYd_aVjeoR",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:16:44.708262Z",
     "start_time": "2025-09-16T16:16:44.699662Z"
    }
   },
   "source": [
    "probs = logits.softmax(dim=-1)\n",
    "values, predictions = probs.topk(5)\n",
    "sequence = tokenizer.decode(predictions)\n",
    "\n",
    "print('Top 5 predictions:', sequence)\n",
    "print(values)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions: box bag bowl jar cup\n",
      "tensor([0.1764, 0.1688, 0.0419, 0.0336, 0.0262], grad_fn=<TopkBackward0>)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vVQ6Cqhuq9t"
   },
   "source": [
    "Printing the top 5 predictions and their respective scores, we see that BERT accurately chooses \"box\" as the most likely replacement for the mask token.\n",
    "\n",
    "## Understanding Causal LM's"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "azFAE8TVkPXv",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:17:09.856060Z",
     "start_time": "2025-09-16T16:16:47.934701Z"
    }
   },
   "source": [
    "## We now repeat a similar exercise with the causal LLM GPT-2. This model generates\n",
    "## text following an input, instead of replacing a mask within the text.\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4TwmbE0bk1mC",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:17:13.881628Z",
     "start_time": "2025-09-16T16:17:13.878244Z"
    }
   },
   "source": [
    "## We can examine the model again, noting the similarities to BERT. An embedding, 12 attention blocks,\n",
    "## and a linear transformation bringing the output back to the size of the tokenizer. The tokenizer is\n",
    "## different from BERT so we see we have more tokens this time.\n",
    "\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hwUCWU6TwTqZ",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:17:16.419060Z",
     "start_time": "2025-09-16T16:17:16.414562Z"
    }
   },
   "source": [
    "## We'll use a different text example, since this model works by producing tokens sequentially\n",
    "## rather than filling a mask.\n",
    "\n",
    "text = \"Swimming at the beach is\"\n",
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "model_inputs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ua8YctwekqSm",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:17:17.463317Z",
     "start_time": "2025-09-16T16:17:17.376881Z"
    }
   },
   "source": [
    "## After applying the model, the information needed to predict the next token is represented by\n",
    "## the last token. So we can access that vector by the index -1.\n",
    "\n",
    "output = model(**model_inputs)\n",
    "next_token_logits = output.logits[:, -1, :]\n",
    "next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "print(next_token)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([257])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OH05O28PnLW9",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:17:18.942267Z",
     "start_time": "2025-09-16T16:17:18.937377Z"
    }
   },
   "source": [
    "## Now add the new token to the end of the text, and feed all of it back to the model to continue\n",
    "## predicting more tokens.\n",
    "\n",
    "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
    "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
    "print(model_inputs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318,   257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-ja9Mk0DnpBe",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:17:20.228807Z",
     "start_time": "2025-09-16T16:17:20.224005Z"
    }
   },
   "source": [
    "## Here's what we have so far. The model added the word 'a' to the input text.\n",
    "\n",
    "print(tokenizer.decode(model_inputs['input_ids'][0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swimming at the beach is a\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n-ygamSIoMKh",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:17:22.426672Z",
     "start_time": "2025-09-16T16:17:22.374629Z"
    }
   },
   "source": [
    "## Repeating all the previous steps, we then add the word 'great'.\n",
    "\n",
    "output = model(**model_inputs)\n",
    "next_token_logits = output.logits[:, -1, :]\n",
    "next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
    "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
    "print(tokenizer.decode(model_inputs['input_ids'][0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swimming at the beach is a great\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z9QIw--gIrh8",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:17:27.797699Z",
     "start_time": "2025-09-16T16:17:27.447264Z"
    }
   },
   "source": [
    "## HuggingFace automates this iterative process. We'll use the quicker approach to finish our sentence.\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_length=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_generate[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swimming at the beach is a great way to get a little extra energy.\n",
      "\n",
      "The beach\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL2rmSMVpkEu"
   },
   "source": [
    "## Pre-training a GPT-2 model from scratch\n",
    "\n",
    "Next we'll train a GPT-2 model from scratch using English Wikipedia data. Note that we're only using a tiny subset of the data to demonstrate that the model is capable of learning. The exact same approach could be followed on the full dataset to train a more functional model, but that would require a lot of compute."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QTW0UmOJd7w2",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:24:31.029930Z",
     "start_time": "2025-09-16T16:17:30.088294Z"
    }
   },
   "source": [
    "# The fix is on this line: use the full Hub ID 'wikimedia/wikipedia'\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "\n",
    "# Adding a seed makes the shuffle operation reproducible\n",
    "ds_shuffle = dataset['train'].shuffle(seed=42)\n",
    "\n",
    "# The rest of your code is correct and remains the same\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_shuffle.select(range(50)),\n",
    "        \"valid\": ds_shuffle.select(range(50, 100))\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the result to see it\n",
    "print(raw_datasets)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 41/41 [06:11<00:00,  9.07s/files]\n",
      "Generating train split: 100%|██████████| 6407814/6407814 [00:41<00:00, 154757.20 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-qbgOvH3d7w2",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:24:44.117566Z",
     "start_time": "2025-09-16T16:24:44.110550Z"
    }
   },
   "source": [
    "print(raw_datasets['train'][0]['text'][:200])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMP Hull is a Category B men's local prison located in Kingston upon Hull in England. The term 'local' means that this prison holds people on remand to the local courts. The prison is operated by His \n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-HRmdiqbd7w3",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:24:47.603631Z",
     "start_time": "2025-09-16T16:24:46.212528Z"
    }
   },
   "source": [
    "## We'll tokenize the text, setting the context size to 128 and thus breaking each document into chunks of 128 tokens.\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 10\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 93, 128, 128, 128, 30]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iWvoQ3-td7w3",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:24:51.221167Z",
     "start_time": "2025-09-16T16:24:51.155305Z"
    }
   },
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1952.33 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1721.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 223\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 248\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfu1oJeQYiLa"
   },
   "source": [
    "Now we can set up the HuggingFace Trainer as follows. Since we're using such a small dataset, we'll need lots of epochs for the model to make progress because all of the parameters are randomly initialized at the outset. Typically, most LLM's are trained for only one epoch and more diverse examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7jf0Le4Yd7w3",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:25:07.664127Z",
     "start_time": "2025-09-16T16:25:06.383215Z"
    }
   },
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O_OKDaPMd7w3",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:25:09.976502Z",
     "start_time": "2025-09-16T16:25:09.973648Z"
    }
   },
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T16:25:13.490382Z",
     "start_time": "2025-09-16T16:25:13.487470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import transformers\n",
    "\n",
    "# Add these lines to the top of your script\n",
    "print(\"Python Executable:\", sys.executable)\n",
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "# ------------------------------------------------\n",
    "\n",
    "# ... your existing code starts here\n",
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Executable: /opt/venv/bin/python3\n",
      "Transformers Version: 4.56.1\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ex3MHkued7w4",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:25:34.075630Z",
     "start_time": "2025-09-16T16:25:33.765599Z"
    }
   },
   "source": [
    "# Correct syntax for NEW versions of transformers\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"data/wiki-gpt2\",\n",
    "    #evaluation_strategy=\"steps\",    # Use this instead of the old argument\n",
    "    eval_steps=500,                 # Optional: Specify evaluation frequency\n",
    "    num_train_epochs=100,\n",
    "    # You might also want to control saving and logging\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=500\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45/2623754309.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T16:25:50.604708Z",
     "start_time": "2025-09-16T16:25:50.599081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "import inspect # <-- Import the inspect module\n",
    "\n",
    "# --- Add this diagnostic code ---\n",
    "print(\"--- Debugging Information ---\")\n",
    "print(\"Python Executable:\", sys.executable)\n",
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "try:\n",
    "    # This will print the file path of the TrainingArguments class being used\n",
    "    print(\"TrainingArguments is from file:\", inspect.getfile(TrainingArguments))\n",
    "except TypeError:\n",
    "    print(\"Could not get file for TrainingArguments. It might be a built-in or dynamically generated class.\")\n",
    "print(\"---------------------------\\n\")\n",
    "# --------------------------------\n",
    "\n",
    "# ... your existing code\n",
    "# args = TrainingArguments(...)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Debugging Information ---\n",
      "Python Executable: /opt/venv/bin/python3\n",
      "Transformers Version: 4.56.1\n",
      "TrainingArguments is from file: /opt/venv/lib/python3.12/site-packages/transformers/training_args.py\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WIj0n1fbd7w4",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:34:21.351130Z",
     "start_time": "2025-09-16T16:25:52.932861Z"
    }
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2800' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2800/2800 08:28, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "0e66423d83c8f556e1c0fc7bb6e2d5b2"
     }
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2800, training_loss=1.4966388651302882, metrics={'train_runtime': 508.2876, 'train_samples_per_second': 43.873, 'train_steps_per_second': 5.509, 'total_flos': 1456703078400000.0, 'train_loss': 1.4966388651302882, 'epoch': 100.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S2avxzYR5U8Y",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:35:14.168066Z",
     "start_time": "2025-09-16T16:35:12.487833Z"
    }
   },
   "source": [
    "trainer.evaluate()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "3462d41d18ddaa58ece73fa10d85c5c6"
     }
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.363317489624023,\n",
       " 'eval_runtime': 1.6761,\n",
       " 'eval_samples_per_second': 147.961,\n",
       " 'eval_steps_per_second': 18.495,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIkCLEFRc3h6"
   },
   "source": [
    "The training loss is low by the end, which means the model should perform very well on training examples it has seen. It does not generalize well to the validation set of course, since we deliberately overfit on a small train set.\n",
    "\n",
    "We can confirm with a couple of examples that were seen in training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x-cxmRLLsGgB",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:35:29.398019Z",
     "start_time": "2025-09-16T16:35:29.395706Z"
    }
   },
   "source": [
    "text = tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids'][:16])\n",
    "print(text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMP Hull is a Category B men's local prison located in Kingston upon Hull\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bbk_WBhMrj_F",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:35:38.164721Z",
     "start_time": "2025-09-16T16:35:38.160901Z"
    }
   },
   "source": [
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "print(model_inputs['input_ids'].shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ipgkkqn4sEAp",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:35:43.062204Z",
     "start_time": "2025-09-16T16:35:42.908701Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
    "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
    "output_generate"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   39,  7378, 28238,   318,   257, 21743,   347,  1450,   338,  1957,\n",
       "          3770,  5140,   287, 34612,  2402, 28238,   287,  4492,    13,   383,\n",
       "          3381,   705, 12001,     6,  1724,   326,   428,  3770,  6622,   661,\n",
       "           319,   816]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JHUbsd-hxdTL",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:35:59.803234Z",
     "start_time": "2025-09-16T16:35:59.796768Z"
    }
   },
   "source": [
    "sequence = tokenizer.decode(output_generate[0])\n",
    "print(sequence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMP Hull is a Category B men's local prison located in Kingston upon Hull in England. The term 'local' means that this prison holds people on rem\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l04YrQLeghA5"
   },
   "source": [
    "The model should do quite well at reciting text after seeing it so many times. We can be convinced that the tokenizer, model architecture, and training objective are well-suited to learning Wikipedia data. For comparison, we'll try this model on text from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MFK5FC57hwS9",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:36:17.267868Z",
     "start_time": "2025-09-16T16:36:17.264310Z"
    }
   },
   "source": [
    "text = tokenizer.decode(tokenized_datasets[\"valid\"][0]['input_ids'][:32])\n",
    "print(text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irma Helena Karvikko (29 September 1909, in Turku – 16 September 1994; surname until 1933 Blomqvist) was a Finnish journalist and\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-SlX_sfHfdTB",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:36:36.317808Z",
     "start_time": "2025-09-16T16:36:36.248336Z"
    }
   },
   "source": [
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
    "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
    "sequence = tokenizer.decode(output_generate[0])\n",
    "print(sequence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irma Helena Karvikko (29 September 1909, in Turku – 16 September 1994; surname until 1933 Blomqvist) was a Finnish journalist and Political Science (After Charles Ray), Doeringer's photographs each contain himself standing\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FassINSdfs9U",
    "ExecuteTime": {
     "end_time": "2025-09-16T16:46:23.487781Z",
     "start_time": "2025-09-16T16:46:23.481228Z"
    }
   },
   "source": [
    "raw_datasets['valid'][0]['text']"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nikolai Petrovich Ostroumov (; 1846–1930) was an imperial Russian orientalist, ethnographer and educationalist in Turkestan.\\n\\nHe studied under Nikolai Il'minskii at the Kazan Theological Seminary, where he studied Arabic and Turkic languages as well as Islam.\\n\\nHe was editor of Turkistan Wilayatining Gazeti from 1883 to 1917.\\n\\nReferences\\n\\nRussian educators\\nTurkestan\\n1846 births\\n1930 deaths\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ivQP0XciLl1"
   },
   "source": [
    "As expected, our model is completely confused this time. We'd need to train for much longer, and on much more diverse data, before we would have a model that can sensibly complete prompts it has never seen before. This is precisely why pre-training is such an important and powerful technique. If we had to train on all of Wikipedia for every NLP application to achieve optimal performance, it would be prohibitively expensive. But there's no need to do that when we can share and reuse existing pre-trained models as we did in the first part of this tutorial."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T16:48:21.199244Z",
     "start_time": "2025-09-16T16:48:21.196254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = tokenizer.decode(tokenized_datasets[\"valid\"][0]['input_ids'][:256])\n",
    "print(text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irma Helena Karvikko (29 September 1909, in Turku – 16 September 1994; surname until 1933 Blomqvist) was a Finnish journalist and politician. She was Deputy Minister for Social Affairs from 17 November 1953 to 4 May 1954 and Minister for Social Affairs from 27 May to 1 September 1957. She was a member of the Parliament of Finland, representing the National Progressive Party from 1948 to 1951, the People's Party of Finland from 1951 to 1958 and from 1962 to 1965 and the Liberal People's Party from 1965 to 1970.\n",
      "\n",
      "References\n",
      "\n",
      "1909 births\n",
      "1994 deaths\n",
      "People from Turku\n",
      "People from Turk\n"
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
